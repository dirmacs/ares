# Cargo workspace configuration
[workspace]
members = [".", "crates/*"]
exclude = ["ui"]
resolver = "2"

[workspace.package]
version = "0.3.3"
edition = "2021"
license = "MIT"
repository = "https://github.com/dirmacs/ares"
authors = ["Dirmacs <build@dirmacs.com>"]

[workspace.dependencies]
# Shared dependencies across workspace
ares-vector = { version = "0.1.1", path = "crates/ares-vector" }
tokio = { version = "1.48", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
thiserror = "2.0"
tracing = "0.1"
parking_lot = "0.12"
uuid = { version = "1.19", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
async-trait = "0.1"

[package]
name = "ares-server"
version.workspace = true
edition.workspace = true
rust-version = "1.91"
description = "A.R.E.S - Agentic Retrieval Enhanced Server: A production-grade agentic chatbot server with multi-provider LLM support, tool calling, RAG, and MCP integration"
license.workspace = true
repository.workspace = true
homepage = "https://github.com/dirmacs/ares"
documentation = "https://docs.rs/ares-server"
readme = "README.md"
keywords = ["llm", "chatbot", "agent", "rag", "ai"]
categories = ["web-programming", "asynchronous", "command-line-utilities"]
authors.workspace = true
exclude = [
    "data/*",
    "target/*",
    ".env",
    ".env.example",
    "*.db",
    "docker-compose*.yml",
    "Dockerfile",
    "hurl/*",
    "scripts/*",
]

# Library configuration - allows other projects to use this as a dependency
[lib]
name = "ares"
path = "src/lib.rs"

# Binary configuration - allows running as standalone server
[[bin]]
name = "ares-server"
path = "src/main.rs"

[features]
# Default features for local-first development
default = ["local-db", "ollama", "ares-vector"]

# ============= LLM Providers =============
# Ollama - Local LLM inference via Ollama server
ollama = ["dep:ollama-rs"]

# OpenAI - OpenAI API and compatible endpoints
openai = ["dep:async-openai"]

# LlamaCpp - Direct GGUF model loading via llama.cpp
llamacpp = ["dep:llama-cpp-2"]

# Anthropic - Claude API
anthropic = ["dep:claude-sdk"]

# LlamaCpp GPU backends (mutually exclusive - pick one)
llamacpp-cuda = ["llamacpp", "llama-cpp-2/cuda"]
llamacpp-metal = ["llamacpp", "llama-cpp-2/metal"]
llamacpp-vulkan = ["llamacpp", "llama-cpp-2/vulkan"]

# ============= Database Backends =============
# Local SQLite database via libsql (default for local development)
local-db = []

# Remote Turso database support
turso = []

# ============= Vector Store Backends =============
# ares-vector - Pure Rust embedded vector database with HNSW (default, local-first)
# No native dependencies, compiles anywhere Rust does
ares-vector = ["dep:ares-vector"]

# LanceDB - Serverless, embedded vector database
# Note: Requires protoc compilation, may have issues on Windows MSVC
lancedb = ["dep:lancedb", "dep:lance"]

# Qdrant vector database for semantic search
qdrant = ["dep:qdrant-client"]

# pgvector - PostgreSQL extension for vector similarity search
pgvector = ["dep:sqlx"]

# ChromaDB - Simple, open-source embedding database
chromadb = ["dep:chromadb"]

# Pinecone - Managed cloud vector database (alpha)
pinecone = ["dep:pinecone-sdk"]

# ============= Additional Features =============
# MCP (Model Context Protocol) server support
mcp = ["dep:rmcp"]

# Local embeddings - fastembed-based ONNX embedding models
# Disable this feature on Windows if you encounter ort_sys linker errors
local-embeddings = ["dep:fastembed"]

# ============= UI =============
# Embedded UI - serves the Leptos frontend from the backend
ui = ["dep:rust-embed", "dep:mime_guess"]

# Swagger UI - interactive API documentation (requires network during build)
swagger-ui = ["dep:utoipa-swagger-ui"]

# ============= Feature Bundles =============
# All LLM providers
all-llm = ["ollama", "openai", "llamacpp", "anthropic"]

# All database backends
all-db = ["local-db", "turso", "qdrant"]

# All vector stores (excluding lancedb due to protoc issues on Windows)
all-vectorstores = ["ares-vector", "qdrant", "pgvector", "chromadb", "pinecone"]

# Local-first vector stores (no external server required)
local-vectorstores = ["ares-vector"]

# Full feature set for development/testing (uses ares-vector instead of lancedb)
full = ["ollama", "openai", "llamacpp", "anthropic", "turso", "qdrant", "ares-vector", "mcp", "swagger-ui", "local-embeddings"]

# Full feature set with UI
full-ui = ["full", "ui"]

# Minimal build - no optional features
minimal = []

[dependencies]
# CLI and TUI
clap = { version = "4.5", features = ["derive", "color", "env", "suggestions"] }
owo-colors = "4.1"

# Core dependencies
anyhow = "1.0.100"
async-stream = "0.3.6"
async-trait = "0.1.89"
chrono = { version = "0.4.42", features = ["serde"] }
futures = "0.3.31"
serde = { version = "1.0.228", features = ["derive"] }
serde_json = "1.0.145"
thiserror = "2.0.17"
tokio = { version = "1.48.0", features = ["full"] }
tracing = "0.1.44"
tracing-subscriber = { version = "0.3.22", features = ["env-filter", "json"] }
uuid = { version = "1.19.0", features = ["v4", "serde"] }

# Web framework
axum = { version = "0.8.7", features = ["macros", "multipart"] }
tower = { version = "0.5.2", features = ["full"] }
tower-http = { version = "0.6.8", features = ["trace", "cors", "compression-gzip", "limit"] }
tower-sessions = "0.14.0"
tower_governor = "0.8"

# HTTP client
reqwest = { version = "0.12.26", features = ["json"] }

# Authentication
argon2 = "0.5.3"
jsonwebtoken = { version = "10.3.0", default-features = false, features = ["aws_lc_rs", "use_pem"] }
sha2 = "0.10.9"
rand = "0.9.2"

# Configuration
config = "0.15.19"
dotenvy = "0.15.7"
toml = "0.9.8"
toon-format = "0.4.1"
notify = { version = "8.2.0", features = ["macos_fsevent"] }
parking_lot = "0.12.5"
arc-swap = "1.7.1"

# Database - libsql (always included for local-db)
libsql = { version = "0.9.29", features = ["core", "replication"] }

# Vector database (optional)
# ares-vector - Pure Rust embedded vector DB (default, recommended)
ares-vector = { workspace = true, features = ["serde"], optional = true }
# External vector DBs
qdrant-client = { version = "1.16.0", optional = true }
lancedb = { version = "0.23.1", optional = true }
# Lance with protoc feature compiles protobuf compiler from source - may have issues on Windows
lance = { version = "1.0.1", features = ["protoc"], default-features = false, optional = true }
sqlx = { version = "0.8.6", features = ["runtime-tokio", "postgres", "tls-rustls"], optional = true }
chromadb = { version = "2.3.0", optional = true }
pinecone-sdk = { version = "0.1.2", optional = true }

# LLM Providers (optional)
ollama-rs = { version = "0.3.3", features = ["stream"], optional = true }
async-openai = { version = "0.31.1", features = ["chat-completion"], optional = true }
llama-cpp-2 = { version = "0.1.130", optional = true }
claude-sdk = { version = "1.0.0", optional = true }



# Embeddings and RAG
fastembed = { version = "5.5.0", optional = true }
# rig-core removed - was unused in codebase
text-splitter = { version = "0.19", features = ["tiktoken-rs"] }
lru = "0.16"

# Web scraping and search
daedra = "0.1.4"
scraper = "0.25.0"

# Schema and documentation
schemars = { version = "1.1.0", features = ["derive"] }
utoipa = { version = "5.4.0", features = ["axum_extras", "chrono", "uuid"] }
utoipa-swagger-ui = { version = "9.0.2", features = ["axum"], optional = true }

# MCP support (optional)
rmcp = { version = "0.12.0", features = ["server", "client", "transport-io", "macros"], optional = true }

# UI embedding (optional)
rust-embed = { version = "8.9", optional = true }
mime_guess = { version = "2.0", optional = true }

[dev-dependencies]
# Testing utilities
axum-test = "18.4.1"
mockall = "0.14.0"
rstest = "0.26.1"
tempfile = "3.23.0"
wiremock = "0.6.5"

# Note: cargo-llvm-cov is a CLI tool, not a library dependency
# Install with: cargo install cargo-llvm-cov

[profile.dev]
# Faster compile times for development
opt-level = 0
debug = true

[profile.release]
opt-level = 3
lto = true
codegen-units = 1
strip = true

[profile.test]
opt-level = 0
debug = true

# Documentation settings
[package.metadata.docs.rs]
# Don't use all-features because llamacpp and qdrant have build scripts
# that fail in docs.rs's sandboxed read-only environment
features = ["ollama", "openai", "local-db", "turso", "mcp"]
rustdoc-args = ["--cfg", "docsrs"]
